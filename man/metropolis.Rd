% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metropolis.r
\name{metropolis}
\alias{metropolis}
\alias{rawMetropolis}
\title{The Metropolis Algorithm}
\usage{
metropolis(init, moves, suffStats = 0, config = matrix(0), iter = 1000,
  burn = 0, thin = 1, dist = c("hypergeometric", "uniform"),
  engine = c("Cpp", "R"), hitAndRun = FALSE, SIS = FALSE,
  nonUniform = FALSE, adaptive = FALSE)

rawMetropolis(init, moves, iter = 1000, dist = "hypergeometric",
  hitAndRun = FALSE, SIS = FALSE, nonUniform = FALSE, adaptive = FALSE)
}
\arguments{
\item{init}{the initial step}

\item{moves}{the moves to be used (the negatives will be added); 
they are arranged as the columns of a matrix.}

\item{suffStats}{the sufficient statistics of the model. Only used when SIS = TRUE. Defaulted to 0.}

\item{config}{the configuration matrix that encodes the model. Only used when SIS = TRUE. Defaulted to matrix(0).}

\item{iter}{number of chain iterations}

\item{burn}{burn-in}

\item{thin}{thinning}

\item{dist}{steady-state distribution; "hypergeometric" (default)
or "uniform"}

\item{engine}{C++ or R? (C++ yields roughly a 20-25x speedup)}

\item{hitAndRun}{Whether or not to use the discrete hit and run algorithm in
the metropolis algorithm. Defaulted to FALSE}

\item{SIS}{If TRUE, with a small probability the move will be chosen randomly from the uniform distribution 
on the fiber using Sequential Importance "Like" Sampling methods. Defaulted to FALSE}

\item{nonUniform}{If TRUE, moves will be chosen adaptively using a move weighting system that uses information 
from previous steps. Defaulted to FALSE}

\item{adaptive}{Option when hitAndRun = TRUE. If adaptive = TRUE, hit and run will choose a proposal distribution adaptively. 
Defaulted to FALSE}
}
\value{
a list
}
\description{
Given a starting table (as a vector) and a collection of moves, 
run the Metropolis-Hastings algorithm starting with the starting 
table.
}
\details{
See Algorithm 1.1.13 in LAS, the reference below.
}
\examples{

\dontrun{

library(ggplot2); theme_set(theme_bw())

# move up and down integer points on the line y = 100 - x
# sampling from the hypergeometric distribution
init <- c(10,90)
moves <- matrix(c(1,-1), ncol = 1)
out <- metropolis(init, moves)
qplot(out$steps[1,])

# view convergence through trace plot
qplot(1:1000, out$steps[1,])

# sampling from the uniform distribution
out <- metropolis(init, moves, dist = "uniform")
qplot(out$steps[1,])

# view convergence through trace plot
qplot(1:1000, out$steps[1,])

# look at autocorrelation
acf(out$steps[1,])
# thin
out <- metropolis(init, moves, dist = "uniform", thin = 2500)
acf(out$steps[1,])
qplot(out$steps[1,])


















data(handy)

exp   <- loglin(handy, as.list(1:2), fit = TRUE)$fit
e <- unname(tab2vec(exp))
h <- t(t(unname(tab2vec(handy))))
chisq <- algstat:::computeX2sCpp(h, e)

out <- loglinear(~ Gender + Handedness, data = handy)
chisqs <- algstat:::computeX2sCpp(out$steps, e)

mean(chisqs >= chisq)
fisher.test(handy)$p.value





A <- hmat(c(2,2), as.list(1:2))
moves <- markov(A)
outC <- metropolis(tab2vec(handy), moves, suffStats = tab2vec(handy) \%*\% A, config = A, 1e4, engine = "Cpp")
str(outC)
outR <- metropolis(tab2vec(handy), moves, suffStats = tab2vec(handy) \%*\% A, config = A, 1e4, engine = "R", thin = 20)
str(outR)

# showSteps(out$steps)


library(microbenchmark)
microbenchmark(
  metropolis(tab2vec(handy), moves, suffStats = tab2vec(handy) \%*\% A, config = A,engine = "Cpp"),
  metropolis(tab2vec(handy), moves, suffStats = tab2vec(handy) \%*\% A, config = A,engine = "R")
)

# cpp ~ 20-25x faster




# examples using the extra options inside metropolis 


data("HairEyeColor")
tbl <- tab2vec(apply(HairEyeColor, c(1, 2), sum))
A <- hmat(c(4,4),1:2)
moves <- markov(A)
suffStats <- A \%*\% tbl

# base metropolis algorithm 
base <- metropolis(tbl, moves, suffStats, A)

# hit and run option
har <- metropolis(tbl, moves, suffStats, A, hitAndRun = TRUE)

# check convergence through trace plots
baseStats <- algstat:::computeUProbsCpp(base$steps)
harStats <- algstat:::computeUProbsCpp(har$steps)

data <- data.frame(baseStats = baseStats, harStats = harStats, steps = 1:1000)

ggplot(data = data) + geom_line(aes(steps, baseStats)) + 
geom_line(aes(steps, harStats), color = "red") + 
labs(x = "Steps", y = "UNLL value", title = "Base Algorithm vs. Algorithm with Hit and Run option in red")


showSteps <- function(steps){
  apply(steps, 2, function(x){
    x <- format(x)
    tab <- vec2tab(x, dim(handy))
    message(
      paste(
        apply(tab, 1, paste, collapse = " "),
        collapse = " "
      )
    )
    message("
", appendLF = F)
  })
  invisible()
}
# showSteps(out$steps)








}


}
\references{
Drton, M., B. Sturmfels, and S. Sullivant (2009). 
  \emph{Lectures on Algebraic Statistics}, Basel: Birkhauser 
  Verlag AG.
}
\author{
David Kahle
}
